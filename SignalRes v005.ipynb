{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "import sqlalchemy as sql\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# my helper functions\n",
    "# -----------------------------------------------------------------------------\n",
    "# use FamaFrench now\n",
    "def ffill2lastobs(x):   \n",
    "    # in one liner, but then removes the last row, if all are NAs\n",
    "    # similar to https://stackoverflow.com/questions/36388419/forward-fill-all-except-last-value-in-python-pandas-dataframe\n",
    "    # return(x.apply(lambda x: x.loc[:x.last_valid_index()].fillna(method='ffill')))\n",
    "    for i in x.columns:\n",
    "        x.loc[:x.loc[:, i].last_valid_index(), i] = x.loc[:x.loc[:, i].last_valid_index(), i].fillna(method='ffill')        \n",
    "    return x\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# settings\n",
    "# -----------------------------------------------------------------------------\n",
    "pd.set_option('display.max_columns', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# global variables\n",
    "# -----------------------------------------------------------------------------\n",
    "io_path = 'SignalRes_005'\n",
    "filepath = 'C:\\\\xyz\\\\tmp_python_files\\\\'\n",
    "\n",
    "# helper fct\n",
    "def back2pickle(x, fname, io_path = io_path):\n",
    "    # backup an object to a pickle file\n",
    "    pd.to_pickle(x, os.path.join(io_path, fname + '.pickle'))\n",
    "    \n",
    "def ChartCumPerf(x, logy=False, **kwargs):\n",
    "    # kwargs: plot() arguments such as color, etc\n",
    "    np.cumprod(1 + x).plot(logy=logy, **kwargs)\n",
    "\n",
    "\n",
    "def TableAnnPerf(x, geom = True):\n",
    "    # we assume all returns are excess of cash (Sharpe Ratio)\n",
    "    periods_pa = 12\n",
    "    \n",
    "    #assert(not(x.isnull().any().any()))\n",
    "    if x.isnull().any().any():\n",
    "        print('the following columns contain NAs')\n",
    "        print(x.columns[x.isnull().any()])\n",
    "        print('NAs will be removed in each column')\n",
    "    # https://stackoverflow.com/questions/20708455/different-results-for-standard-deviation-using-numpy-and-r\n",
    "    # todo np.std(x1, axis = 0, ddof = 1)\n",
    "    if geom:\n",
    "        print('geometric chaining is used to aggregate returns')\n",
    "        # remove NAs in len(x.dropna()), since np.prod() does deal with NA (removes), but not len    \n",
    "        ret_pa = x.apply(lambda x: np.prod(1 + x.dropna())**(periods_pa/len(x.dropna())) - 1, axis = 0)\n",
    "        sd_pa = x.apply(lambda x: np.std(x.dropna(), ddof = 1) * np.sqrt(periods_pa))\n",
    "\n",
    "    if not geom:\n",
    "        print('arithmetic chaining is used to aggregate returns')\n",
    "        # remove NAs in len(x.dropna()), since np.prod() does deal with NA (removes), but not len    \n",
    "        ret_pa = x.apply(lambda x: x.dropna().mean() * periods_pa, axis = 0)\n",
    "        sd_pa = x.apply(lambda x: np.std(x.dropna(), ddof = 1) * np.sqrt(periods_pa))\n",
    "\n",
    "\n",
    "    retval = pd.DataFrame(np.array([ret_pa, sd_pa, ret_pa / sd_pa]), \n",
    "                          columns = x.columns, \n",
    "                          index = ['Return annualized', 'Std Dev annualized', \n",
    "                                     'Sharpe ratio annualized'])\n",
    "    return(retval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# import data (watch out: wide and long formats, and need to reshape)\n",
    "# (1) import from SQL as long format\n",
    "# (2) pivot to wide to do ffill, asfreq, etc. and vectorized calc like \n",
    "#     return calc, rolling fct, or shift() for FwdRet, and ratios like e.g. p/e \n",
    "# (3) stack (melt) to long form and finally merge into yx df \n",
    "#     for analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# works but let's assume done in another place and saved to pickle files\n",
    "if False:    \n",
    "    # connect to MySQL db and import long format monthly data\n",
    "    connect_string = 'mysql://Chrigoo:missql@localhost/pythonspot'\n",
    "    sql_engine = sql.create_engine(connect_string)\n",
    "    sql_engine.execute('show tables').fetchall() # show all tables\n",
    "    #sql_engine.execute('SELECT COUNT(ticker) FROM totret').fetchall() \n",
    "    # how many rows of monthly prices: 1.3mn\n",
    "    sql_engine.execute('SELECT COUNT(ticker) FROM totret WHERE DATE( date ) = LAST_DAY( date )').fetchall() \n",
    "    \n",
    "    # import total returns (level, price) AS MONTHLY\n",
    "    query_string = \"SELECT * FROM totret WHERE DATE( date ) = LAST_DAY( date )\"\n",
    "    t1_start = time.perf_counter()  \n",
    "    cursor = sql_engine.execute(query_string)\n",
    "    rows = cursor.fetchall()\n",
    "    #d_long_import = pd.DataFrame(rows)\n",
    "    col_names = pd.DataFrame(sql_engine.execute('SHOW COLUMNS FROM totret').fetchall())[0]\n",
    "    d_long_import = pd.DataFrame(rows, columns=col_names)\n",
    "    t1_stop = time.perf_counter()\n",
    "    print(\"Elapsed time using PyMySQL fetchall:\", t1_stop - t1_start) # 57s for 1.3mn rows of monthly data\n",
    "    sql_engine.dispose()\n",
    "    \n",
    "    #back2pickle(d_long_import, 'd_long_import') \n",
    "    \n",
    "    print(d_long_import.head())\n",
    "    d_long_import.shape\n",
    "    # great, correct data types for columns\n",
    "    d_long_import.dtypes # especially datetime64\n",
    "    \n",
    "    # reshape long totret (price level) to wide, ffill and calc monthly ret\n",
    "    totret = d_long_import.pivot('date', 'ticker', 'TotRet')\n",
    "    del(d_long_import)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#******************************************************************************\n",
    "#******************************************************************************\n",
    "# v005 read pickle saved in xyz_sql v004\n",
    "sql_pickle_path = 'C:\\\\Users\\\\xyz\\\\Documents\\\\Python Scripts\\\\xyzSQL\\\\'\n",
    "\n",
    "def import_sql_pickle(field):\n",
    "    d = pd.read_pickle(os.path.join(sql_pickle_path, field + '.pickle'))\n",
    "    d.rename(columns={'tradingitemid': 'ticker'}, inplace=True)\n",
    "    d = d.pivot('date', 'ticker', field) # rename tradingitemid to ticker\n",
    "    # convert tradingitemid from int to string (like ticker), in case below code\n",
    "    # assumes it to be a string\n",
    "    d.columns = d.columns.astype(str)\n",
    "    return(d)\n",
    "\n",
    "# priceCloseNotAdj to calc market cap in case tot_sh_outs is not adjusted\n",
    "priceCloseNotAdj = import_sql_pickle('priceCloseNotAdj')\n",
    "priceCloseNotAdj = ffill2lastobs(priceCloseNotAdj).asfreq('M')\n",
    "# if 'daily', we might need to forward fill\n",
    "tot_sh_outs = import_sql_pickle('tot_sh_outs')\n",
    "tot_sh_outs = ffill2lastobs(priceCloseNotAdj).asfreq('M')\n",
    "mktcap = priceCloseNotAdj * tot_sh_outs\n",
    "del(priceCloseNotAdj, tot_sh_outs)\n",
    "\n",
    "# priceCloseAdj to calc totret\n",
    "priceCloseAdj = import_sql_pickle('priceCloseAdj')\n",
    "# *******\n",
    "totret = priceCloseAdj\n",
    "# *******\n",
    "del(priceCloseAdj)\n",
    "\n",
    "# is gross_profit LTM or last filing?\n",
    "# otherwise Gross margin is the difference between revenue and cost of goods sold divided by revenue\n",
    "gross_profit = import_sql_pickle('gross_profit')\n",
    "tot_assets = import_sql_pickle('tot_assets')\n",
    "\n",
    "# not necessarily given for sql data, but pandas will do division correctly\n",
    "#assert(all(gross_profit.index == tot_assets.index))\n",
    "#assert(all(gross_profit.columns == tot_assets.columns))\n",
    "\n",
    "profitab = gross_profit / tot_assets\n",
    "#profitab.index.name = 'date' # to be consistent with data from MySQL\n",
    "#profitab.rename_axis(\"ticker\", axis=\"columns\", inplace=True)\n",
    "del(gross_profit, tot_assets)\n",
    "\n",
    "bics = pd.read_pickle(os.path.join(sql_pickle_path, \"sectorname\" + '.pickle'))\n",
    "bics.index = bics.tradingitemid.astype(str)\n",
    "bics.rename(columns = {'sectorname':'BICS_sec'}, inplace=True)\n",
    "bics.rename_axis(\"ID\", axis=\"rows\", inplace=True)\n",
    "bics.drop(['tradingitemid'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "cntry_of_dom = pd.read_pickle(os.path.join(sql_pickle_path, \"isoCountry2\" + '.pickle'))\n",
    "cntry_of_dom.index = cntry_of_dom.tradingitemid.astype(str)\n",
    "cntry_of_dom.rename(columns = {'isoCountry2':'cntry_of_dom'}, inplace=True)\n",
    "cntry_of_dom.rename_axis(\"ID\", axis=\"rows\", inplace=True)\n",
    "cntry_of_dom.drop(['tradingitemid'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#******************************************************************************\n",
    "#******************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "# if above xyz sql import doesn't work for whatever reason:\n",
    "if False:\n",
    "    # ---- TEMP totret from pickle\n",
    "    totret = pd.read_pickle(os.path.join(os.path.join(filepath, dir_stem + '_' + 'fullperiod'), \n",
    "                                    \"TotRet\" + '_data.pickle'))\n",
    "    totret.shape # (6574, 6156)\n",
    "    totret.index.name = 'date'\n",
    "    totret.columns.name = 'ticker' # was 'tickers'\n",
    "# ---- END TEMP\n",
    "\n",
    "totret = ffill2lastobs(totret).asfreq('M')\n",
    "#totret.apply(lambda x: x.isnull().all(), axis = 0).sum() # 41 cols are all na\n",
    "\n",
    "# show number of non-missing totret over time\n",
    "#totret.apply(lambda x: (~np.isnan(x)).sum(), axis = 1).plot()\n",
    "totret = totret.pct_change() # log returns (not additive over assets!): np.log(1 + totret.pct_change)\n",
    "#assert(all(totret.index == rf_m.index)) # \n",
    "#back2pickle(totret, 'totret') \n",
    "#****************************\n",
    "#****************************\n",
    "# TODO: CALC EXCESS RETURNS\n",
    "#****************************\n",
    "#****************************\n",
    "\n",
    "#back2pickle(totret, 'totret') \n",
    "#totret = pd.read_pickle(os.path.join(io_path, 'totret.pickle'))\n",
    "\n",
    "# FwdRet: shift totret forward 1 period\n",
    "fwdret = totret.shift(-1)   \n",
    "#back2pickle(fwdret, 'fwdret') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# if above xyz sql import doesn't work for whatever reason:\n",
    "if False:\n",
    "    \n",
    "    # import ProfitabLTM and TotAsset0 wide format\n",
    "    pltm = pd.read_pickle(os.path.join(os.path.join(filepath, dir_stem + '_' + 'fullperiod'), \"GrossProfitLTM\" + '_data.pickle'))\n",
    "    pltm = pltm.fillna(method = 'ffill', limit = 380).asfreq('M') \n",
    "    totasset0 = pd.read_pickle(os.path.join(os.path.join(filepath, dir_stem + '_' + 'fullperiod'), \"TotAsset0Q\" + '_data.pickle'))\n",
    "    totasset0 = totasset0.fillna(method = 'ffill', limit = 380).asfreq('M') \n",
    "    \n",
    "    # we can also calc ratios using long form, but easier in wide (long: date, ticker, value)  \n",
    "    #pltm = (pltm.stack(dropna=False).reset_index()\n",
    "    #            .rename(columns={'level_0': 'date', 'tickers': 'ticker', \n",
    "    #                                0: 'pltm'}))\n",
    "    #totasset0 = (totasset0.stack(dropna=False).reset_index()\n",
    "    #            .rename(columns={'level_0': 'date', 'tickers': 'ticker', \n",
    "    #                                0: 'totasset0'}))\n",
    "    #\n",
    "    #(pltm.pltm / totasset0.totasset0) # equal to profitab\n",
    "    \n",
    "    assert(all(pltm.index == totasset0.index))\n",
    "    assert(all(pltm.columns == totasset0.columns))\n",
    "    profitab = pltm / totasset0\n",
    "    profitab.index.name = 'date' # to be consistent with data from MySQL\n",
    "    profitab.rename_axis(\"ticker\", axis=\"columns\", inplace=True)\n",
    "    del(pltm, totasset0)\n",
    "    #back2pickle(profitab, 'profitab') \n",
    "    # profitab = pd.read_pickle(os.path.join(io_path, 'profitab.pickle'))\n",
    "    \n",
    "    mktcap = pd.read_pickle(os.path.join(os.path.join(filepath, dir_stem + '_' + 'fullperiod'), \"MktCap\" + '_data.pickle'))\n",
    "    mktcap = ffill2lastobs(mktcap).asfreq('M') \n",
    "    mktcap.index.name = 'date'\n",
    "    mktcap.rename_axis(\"ticker\", axis=\"columns\", inplace=True)\n",
    "    #back2pickle(mktcap, 'mktcap') \n",
    "    \n",
    "    ## reshape all to long and then merge using reduce()\n",
    "    #totret = (totret.stack(dropna=False).reset_index()\n",
    "    #            .rename(columns={'level_0': 'date', 'tickers': 'ticker', \n",
    "    #                                0: 'totret'}))\n",
    "    #profitab = (profitab.stack(dropna=False).reset_index()\n",
    "    #            .rename(columns={'level_0': 'date', 'tickers': 'ticker', \n",
    "    #                                0: 'profitab'}))\n",
    "    #mktcap = (mktcap.stack(dropna=False).reset_index()\n",
    "    #            .rename(columns={'level_0': 'date', 'tickers': 'ticker', \n",
    "    #                                0: 'mktcap'}))\n",
    "    #fwdret = (fwdret.stack(dropna=False).reset_index()\n",
    "    #            .rename(columns={'level_0': 'date', 'tickers': 'ticker', \n",
    "    #                                0: 'fwdret'}))\n",
    "     \n",
    "    # import bics\n",
    "    bics = pd.read_pickle(os.path.join(os.path.join(filepath, dir_stem), \"DM_BICS_sec\" + '_data.pickle'))\n",
    "    cntry_of_dom = pd.read_pickle(os.path.join(os.path.join(filepath, dir_stem), \"DM_CNTRY_OF_DOMICILE\" + '_data.pickle'))\n",
    "    #back2pickle(bics, 'bics') \n",
    "    #back2pickle(cntry_of_dom, 'cntry_of_dom') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# alternatively to melt all in one go and then merge into df (yx):    \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "## since tmp totret was sourced from MySQL, date has 'date' column name\n",
    "## but profitab related data currently from pickle: must change also to name 'index\n",
    "## we want to use MySQL in future which will have col name 'date' for date\n",
    "#profitab.index.name = 'date'\n",
    "#mktcap.index.name = 'date'\n",
    "\n",
    "var_dict = {'fwdret': fwdret, 'profitab':profitab, 'mktcap':mktcap}    \n",
    "# Melt all the delta dataframes and store in list\n",
    "# .melt() stacks also cols where all values are nan, similar to \n",
    "# .stack(dropna=False)\n",
    "# !!\n",
    "melted_dfs = []\n",
    "for key, var_df in var_dict.items():\n",
    "    # reset_index will add current index as column called 'index'\n",
    "    melted_dfs.append( var_df.reset_index().melt(id_vars=['date'], value_name=key) )\n",
    "\n",
    "# here we could add a base df and then create a list with all the feature dataframes\n",
    "#feature_dfs = [base_df] + melted_dfs\n",
    "feature_dfs = melted_dfs\n",
    "del(melted_dfs)\n",
    "#back2pickle(feature_dfs, 'feature_dfs') \n",
    "\n",
    "# !!\n",
    "from functools import reduce   \n",
    "df = reduce(lambda left,right: pd.merge(left,right,on=['date', 'ticker']), feature_dfs)\n",
    "#back2pickle(df, 'df') \n",
    "\n",
    "# merge bics and country to it, bics is df, and we need series, so do df['var']\n",
    "# !!\n",
    "df['bics'] = df['ticker'].map(bics['BICS_sec'])\n",
    "df['cntry_of_dom'] = df['ticker'].map(cntry_of_dom['cntry_of_dom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# pooled regression without sector,country neutralisation\n",
    "# only filter: min mktcap 150m\n",
    "# -----------------------------------------------------------------------------\n",
    "# !!\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#df[df.mktcap >= 150e6].shape\n",
    "#df_f = df[df.mktcap >= 150e6].dropna().copy()\n",
    "\n",
    "# todo: filter for US only cntry_of_dom 'US'\n",
    "df_f = df[(df.mktcap >= 150e6) \n",
    "          & (df.bics != 'Financials')\n",
    "          & (df.cntry_of_dom == 'US')].dropna().copy()\n",
    "# cap both y and X at 2 standard deviations to avoid heavy influence of outliers\n",
    "#back2pickle(df_f, 'df_f') \n",
    "\n",
    "outlier_cap = 3\n",
    "#df_f.fwdret = np.clip(df_f.fwdret, -outlier_cap * np.std(df_f.fwdret), \n",
    "#                      outlier_cap * np.std(df_f.fwdret))\n",
    "# !!\n",
    "df_f.profitab = np.clip(df_f.profitab, -outlier_cap * np.std(df_f.profitab, axis=0), \n",
    "                        outlier_cap * np.std(df_f.profitab, axis=0))\n",
    "\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "results = smf.ols(\"fwdret ~ profitab\", data=df_f).fit()\n",
    "print(results.summary())\n",
    "#                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "#------------------------------------------------------------------------------\n",
    "#Intercept      0.0079      0.000     28.826      0.000       0.007       0.008\n",
    "#profitab       0.0055      0.001      8.732      0.000       0.004       0.007\n",
    "#R-squared:                       0.000\n",
    "\n",
    "# todo: ic or simply similar to Jagannthan do xs regression and fama macbeth\n",
    "# Gelman: secret weapon\n",
    "#\n",
    "#from statsmodels.datasets import grunfeld\n",
    "#data = grunfeld.load_pandas().data\n",
    "#data.year = data.year.astype(np.int64)\n",
    "\n",
    "# couldn't install linearmodels with conda\n",
    "#from statsmodels.datasets import grunfeld\n",
    "#data = grunfeld.load_pandas().data\n",
    "#data.year = data.year.astype(np.int64)\n",
    "#from linearmodels import PanelOLS\n",
    "#etdata = data.set_index(['firm','year'])\n",
    "#PanelOLS(etdata.invest,etdata[['value','capital']],entity_effect=True).fit(debiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# quick backtest\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# profitab 30% and 70% breakdown \n",
    "# !!\n",
    "df_f_profitab = ( df_f.groupby(['date'])['profitab']\n",
    "                      .describe(percentiles=[0.3, 0.7]).reset_index() )\n",
    "df_f_profitab = ( df_f_profitab[['date','30%','70%']]\n",
    "                   .rename(columns={'30%':'profitab30', '70%':'profitab70'}) )\n",
    "df_f_profitab.head()\n",
    "# check ok\n",
    "# df_f[df_f.date == '2001-02-28'].profitab.describe(percentiles=[0.3, 0.7])\n",
    "\n",
    "\n",
    "# join back factor (profitab here) breakdown\n",
    "df_f_breaks = pd.merge(df_f, df_f_profitab, how='inner', on=['date'])\n",
    "df_f_breaks.head()\n",
    "# check ok\n",
    "# df_f_breaks1[df_f_breaks1.date == '2001-02-28'].head() ' 30% 0.212853\n",
    "\n",
    "## same as using .map if we set index of df_f_profitab to 'date'\n",
    "#df_f_profitab.index = df_f_profitab.date\n",
    "#df_f_breaks2 = df_f.copy()\n",
    "#df_f_breaks2['profitab30'] = df_f['date'].map(df_f_profitab['profitab30'])\n",
    "## not sure .map can used to df (series as per doc)\n",
    "##df_f_breaks2[['profitab30', 'profitab70']] = df_f['date'].map(df_f_profitab[['profitab30', 'profitab70']])\n",
    "## yes identical to using merge()\n",
    "## df_f_breaks1.head()\n",
    "## df_f_breaks2.sort_values(['date', 'ticker']).head()\n",
    "\n",
    "\n",
    "# -------- v005 new\n",
    "# percentiles for each date AND within each bics: sector neutral\n",
    "df_f_profitab2 = ( df_f.groupby(['date', 'bics'])['profitab']\n",
    "                       .describe(percentiles=[0.3, 0.7]).reset_index() )\n",
    "df_f_profitab2 = ( df_f_profitab2[['date', 'bics','30%','70%']]\n",
    "                     .rename(columns={'30%':'profitab30', '70%':'profitab70'}) )\n",
    "# then .map to df_f using \n",
    "#df_f_breaks2['profitab30'] = df_f[['date', 'bics']].map(df_f_profitab2['profitab30'])\n",
    "df_f_breaks2 = pd.merge(df_f, df_f_profitab2, how='inner', on=['date', 'bics'])\n",
    "\n",
    "## test, ok\n",
    "## MSFT, Technology\n",
    "#df_f_breaks2[(df_f_breaks2.date.isin(['2001-01-31', '2001-02-28', '2001-03-31'])) \n",
    "#                 & (df_f_breaks2.bics=='Technology')\n",
    "#                 & (df_f_breaks2.ticker=='MSFT US Equity')]\n",
    "#df_f_profitab2[df_f_profitab2.bics=='Technology'].head(3)\n",
    "##PEP US Equity, Consumer staples\n",
    "#df_f_breaks2[(df_f_breaks2.date.isin(['2001-01-31', '2001-02-28', '2001-03-31'])) \n",
    "#                 & (df_f_breaks2.bics=='Consumer Staples')\n",
    "#                 & (df_f_breaks2.ticker=='PEP US Equity')]\n",
    "#df_f_profitab2[df_f_profitab2.bics=='Consumer Staples'].head(3)\n",
    "#\n",
    "## 30percentil value correct\n",
    "#( df_f[(df_f.date=='2001-02-28') & (df_f.bics=='Consumer Staples')]\n",
    "#                .profitab.describe(percentiles=[0.3, 0.7]) )\n",
    "\n",
    "# --------- v005 end new\n",
    "df_f_breaks = df_f_breaks2\n",
    "\n",
    "def profitab_bucket(row):\n",
    "    if 0<=row['profitab']<=row['profitab30']:\n",
    "        value = 'L'\n",
    "    elif row['profitab']<=row['profitab70']:\n",
    "        value='M'\n",
    "    elif row['profitab']>row['profitab70']:\n",
    "        value='H'\n",
    "    else:\n",
    "        value=''\n",
    "    return value\n",
    "\n",
    "df_f_breaks['profitab_port'] = df_f_breaks.apply(profitab_bucket, axis=1)\n",
    "# checked, ok \n",
    "df_f_breaks.head()\n",
    "\n",
    "# equal weighted stocks\n",
    "ewret = ( df_f_breaks.groupby(['date', 'profitab_port'])['fwdret']\n",
    "                     .mean().reset_index().rename(columns={'fwdret': 'ewret'}) )\n",
    "\n",
    "\n",
    "# function to calculate value weighted return\n",
    "def wavg(group, avg_name, weight_name):\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "# value-weigthed return\n",
    "vwret = ( df_f_breaks.groupby(['date', 'profitab_port'])\n",
    "                      .apply(wavg, 'fwdret', 'mktcap')\n",
    "                      .to_frame().reset_index().rename(columns={0: 'vwret'}) )\n",
    "\n",
    "#def ewavg(group, avg_name):\n",
    "#    d = group[avg_name]\n",
    "#    try:\n",
    "#        return np.mean(d)\n",
    "#    except ZeroDivisionError:\n",
    "#        return np.nan\n",
    "#\n",
    "#ewret = ( df_f_breaks.groupby(['date', 'profitab_port'])\n",
    "#                      .apply(ewavg, 'fwdret').to_frame()\n",
    "#                      .reset_index().rename(columns={0: 'ewret'}) )\n",
    "\n",
    "# ewret1.equals(ewret)\n",
    "\n",
    "# value weighted stocks\n",
    "ls_port_vw = vwret.pivot(index='date', columns='profitab_port', values='vwret').reset_index()\n",
    "ls_port_vw.index = ls_port_vw.date\n",
    "ls_port_vw = ls_port_vw[ls_port_vw.columns[~ls_port_vw.columns.isin(['date'])]]\n",
    "\n",
    "# long-short strategy (excess returns by construction)\n",
    "ls_vw = ls_port_vw['H'] - ls_port_vw['L']\n",
    "ls_vw.name = 'LS'\n",
    "ChartCumPerf(ls_vw)\n",
    "\n",
    "ChartCumPerf(ls_port_vw)\n",
    "ChartCumPerf(ls_port_vw.loc['2007':])\n",
    "TableAnnPerf(ls_port_vw['2007':])\n",
    "\n",
    "# equal weighted stocks\n",
    "ls_port_ew = ewret.pivot(index='date', columns='profitab_port', values='ewret').reset_index()\n",
    "ls_port_ew.index = ls_port_ew.date\n",
    "ls_port_ew = ls_port_ew[ls_port_ew.columns[~ls_port_ew.columns.isin(['date'])]]\n",
    "\n",
    "# long-short strategy (excess returns by construction)\n",
    "ls_ew = ls_port_ew['H'] - ls_port_ew['L']\n",
    "ls_ew.name = 'LS'\n",
    "ChartCumPerf(ls_ew)\n",
    "\n",
    "ChartCumPerf(ls_port_ew)\n",
    "ChartCumPerf(ls_port_ew.loc['2007':])\n",
    "TableAnnPerf(ls_port_ew['2007':])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Fama French alpha\n",
    "# -----------------------------------------------------------------------------\n",
    "import pandas_datareader.data as web  # module for reading datasets directly from the web\n",
    "from pandas_datareader.famafrench import get_available_datasets\n",
    "from pandas.tseries.offsets import MonthEnd # to convert \"%Y-%m\" to \"%Y-%m-%d\"\n",
    "\n",
    "datasets = get_available_datasets()\n",
    "print('No. of datasets:{0}'.format(len(datasets)))\n",
    "df_5_factor = [dataset for dataset in datasets if '5' in dataset and 'Factor' in dataset]\n",
    "print(df_5_factor)\n",
    "# Taking [0] as extracting 1F-F-Research_Data_Factors_2x3')\n",
    "ds_factors = web.DataReader(df_5_factor[0],'famafrench',start='1963-07-01') # we can add end='2019-06-30' \n",
    "print('\\nKEYS\\n{0}'.format(ds_factors.keys()))\n",
    "print('DATASET DESCRIPTION \\n {0}'.format(ds_factors['DESCR']))\n",
    "#ds_factors[0].head()\n",
    "dfFactor = ds_factors[0].copy()/100\n",
    "dfFactor.rename(columns={'Mkt-RF':'MktRF'}, inplace=True)\n",
    "dfFactor.index = pd.to_datetime(dfFactor.index.to_timestamp(), format=\"%Y-%m\") + MonthEnd(1)\n",
    "dfFactor.head()\n",
    "dfFactor.tail()\n",
    "\n",
    "ChartCumPerf(dfFactor[['MktRF', 'SMB', 'HML', 'RMW', 'CMA']])\n",
    "ChartCumPerf(dfFactor.loc['2003':,['MktRF', 'SMB', 'HML', 'RMW', 'CMA']])\n",
    "TableAnnPerf(dfFactor.loc['2003':,['MktRF', 'SMB', 'HML', 'RMW', 'CMA']])\n",
    "np.round(dfFactor.loc['2003':,['MktRF', 'SMB', 'HML', 'RMW', 'CMA']].corr(), 2)\n",
    "\n",
    "assert(ls_port_ew.index.isin(dfFactor.index).all())\n",
    "ls_port_ew_er = ls_port_ew.subtract(dfFactor.RF, 0).dropna()\n",
    "ls_port_vw_er = ls_port_vw.subtract(dfFactor.RF, 0).dropna()\n",
    "#ls_port_ew_er = ( ls_port_ew.subtract(dfFactor\n",
    "#                    .loc[dfFactor.index.isin(ls_port_ew.index), 'RF'], 0) )\n",
    "\n",
    "\n",
    "TableAnnPerf(ls_port_ew['2007':])\n",
    "TableAnnPerf(ls_port_ew_er['2007':])\n",
    "\n",
    "\n",
    "# regression\n",
    "# merge dataframes\n",
    "#m = pd.merge(rets,f,left_index=True,right_index=True)\n",
    "import statsmodels.formula.api as sm\n",
    "d = pd.concat([ls_ew, ls_port_ew_er, \n",
    "               dfFactor[['MktRF', 'SMB', 'HML', 'RMW', 'CMA']]], axis=1).dropna()\n",
    "d = pd.concat([ls_vw, ls_port_vw_er, \n",
    "               dfFactor[['MktRF', 'SMB', 'HML', 'RMW', 'CMA']]], axis=1).dropna()\n",
    "TableAnnPerf(d['2007':])\n",
    "# compare 5yr SR for LS vw with Novy-Marx JFE 2013 fig 1, though he used quintiles\n",
    "# rolling 5yr SR (we already have excess returns)\n",
    "roll_sr = d['LS'].rolling(60).apply(lambda x: \n",
    "                            np.sqrt(12) * (x.mean() - 0) / x.std(), \n",
    "                            raw = True).plot()\n",
    "roll_sr = d['RMW'].rolling(60).apply(lambda x: \n",
    "                            np.sqrt(12) * (x.mean() - 0) / x.std(), \n",
    "                            raw = True).plot()\n",
    "\n",
    "\n",
    "#sm.ols( formula = \"H ~ MktRF + SMB + HML\", data=d).fit().summary()\n",
    "#sm.ols( formula = \"H ~ MktRF + SMB + HML + RMW + CMA\", data=d).fit().summary()\n",
    "#sm.ols( formula = \"M ~ MktRF + SMB + HML\", data=d).fit().summary()\n",
    "#sm.ols( formula = \"M ~ MktRF + SMB + HML + RMW + CMA\", data=d).fit().summary()\n",
    "#sm.ols( formula = \"L ~ MktRF + SMB + HML\", data=d).fit().summary()\n",
    "#sm.ols( formula = \"L ~ MktRF + SMB + HML + RMW + CMA\", data=d).fit().summary()\n",
    "\n",
    "sm.ols( formula = \"LS ~ MktRF + SMB + HML\", data=d).fit().summary()\n",
    "sm.ols( formula = \"LS ~ MktRF + SMB + HML + RMW + CMA\", data=d).fit().summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
